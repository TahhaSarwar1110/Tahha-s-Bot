{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4a288f90-e5e0-4739-8d29-532586e85490",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "beb2202e-cc4f-4f8a-8b77-262d40536a5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "%dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ae0a38a-9922-4882-91c1-e8f01a9025ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\DELL\\AppData\\Local\\Temp\\ipykernel_4716\\2571472939.py:44: LangChainDeprecationWarning: Please see the migration guide at: https://python.langchain.com/docs/versions/migrating_memory/\n",
      "  chat_memory = ConversationSummaryMemory(llm=ChatOpenAI(), memory_key='message_log')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chatbot is ready! Type 'exit' to end the conversation.\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "You:  One of my colleagues forgot to clock in for one of his shifts. How do I manually add time sheet for the coworker? I have employee permission.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Taha's Bot: As an employee, you do not have the authority to add time sheets for other coworkers. You should contact your management and request that the time sheet be added manually.\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "You:  One of my colleagues forgot to clock in for one of his shifts. How do I manually add time sheet for the coworker? I have employee permission.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Taha's Bot: As an employee, you do not have the authority to add time sheets for other coworkers. Please contact your management and request that the time sheet be added manually.\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "You:  I want to edit the shifts for the whole week.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Taha's Bot: Could you please provide more details about the specific changes you want to make? It's essential to understand your requirements thoroughly before proposing a solution.\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "You:  While exporting reports, is there a way to add the email field in one of the columns?\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Taha's Bot: Yes, you can add the email field in one of the columns while exporting reports. You can generate a custom report and include the email as one of the columns. However, please note that you'll need to manually add the emails after downloading the report.\n"
     ]
    }
   ],
   "source": [
    "# necessary libraries\n",
    "from langchain.memory import ConversationSummaryMemory\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough, RunnableLambda\n",
    "from operator import itemgetter\n",
    "from langchain_core.runnables import chain\n",
    "\n",
    "# RAG setup libraries\n",
    "from langchain_openai.embeddings import OpenAIEmbeddings\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_core.documents import Document\n",
    "from langchain_text_splitters.markdown import MarkdownHeaderTextSplitter\n",
    "from langchain_text_splitters.character import CharacterTextSplitter\n",
    "from langchain_community.document_loaders import Docx2txtLoader, PyPDFLoader\n",
    "\n",
    "\n",
    "page = PyPDFLoader('User Query.pdf')\n",
    "my_document = page.load()\n",
    "\n",
    "#header_splitter = MarkdownHeaderTextSplitter(\n",
    "   # headers_to_split_on=[(\"#\", \"Course Title\"), (\"##\", \"Lecture Title\")]\n",
    "#)\n",
    "#header_splitted_document = header_splitter.split_text(my_document[0].page_content)\n",
    "#for i in range(len(header_splitted_document)):\n",
    "    #header_splitted_document[i].page_content = ' '.join(header_splitted_document[i].page_content.split())\n",
    "\n",
    "character_splitter = CharacterTextSplitter(separator=\".\", chunk_size=500, chunk_overlap=50)\n",
    "character_splitted_documents = character_splitter.split_documents(my_document)\n",
    "for i in range(len(character_splitted_documents)):\n",
    "    character_splitted_documents[i].page_content = ' '.join(character_splitted_documents[i].page_content.split())\n",
    "\n",
    "embedding = OpenAIEmbeddings(model=\"text-embedding-ada-002\")\n",
    "vector_store = Chroma.from_documents(\n",
    "    embedding=embedding,\n",
    "    documents=character_splitted_documents,\n",
    "    persist_directory=\"./TCP_directory_1\"\n",
    ")\n",
    "\n",
    "retriever = vector_store.as_retriever(search_type='mmr', search_kwargs={'k': 3, 'lambda_multi': 0.5})\n",
    "\n",
    "#chatbot memory\n",
    "chat_memory = ConversationSummaryMemory(llm=ChatOpenAI(), memory_key='message_log')\n",
    "\n",
    "#prompt template for the chatbot\n",
    "TEMPLATE = \"\"\"\n",
    "The AI should understand the question completely and only answer strictly based on the provided document context, but it should not talk about document in the answer.  \n",
    "The AI should not hallucinate.\n",
    "if the question does not have enough information, the AI should ask for more details.\n",
    "The AI should Deliver well-structured, polished responses that enhance the overall user experience.\n",
    "If the answer is not available in the context, the AI truthfully responds: \"Sorry, I don't know the answer.\"\n",
    "\n",
    "Current Conversation:\n",
    "{message_log}\n",
    "\n",
    "Question Context:\n",
    "{context}\n",
    "\n",
    "Human:\n",
    "{question}\n",
    "\n",
    "AI:\n",
    "\"\"\"\n",
    "prompt_template = PromptTemplate.from_template(template=TEMPLATE)\n",
    "\n",
    "# ChatOpenAI instance\n",
    "chat = ChatOpenAI(\n",
    "    model=\"gpt-4\",\n",
    "    temperature=0,\n",
    "    max_tokens=250\n",
    ")\n",
    "\n",
    "#chain combining memory and RAG\n",
    "@chain\n",
    "def memory_rag_chain(question):\n",
    "    # Retrieve relevant documents from the vector store\n",
    "    retrieved_docs = retriever.invoke(question)\n",
    "    context = \"\\n\".join([doc.page_content for doc in retrieved_docs])\n",
    "\n",
    "    # If no relevant context is found, a \"Sorry\" response is returned\n",
    "    if not context.strip():\n",
    "        response = \"Sorry, I don't know the answer.\"\n",
    "        chat_memory.save_context(inputs={'input': question}, outputs={'output': response})\n",
    "        return response\n",
    "\n",
    "    # Combining memory and RAG for the prompt\n",
    "    chain = (\n",
    "        RunnablePassthrough.assign(\n",
    "            message_log=RunnableLambda(chat_memory.load_memory_variables) | itemgetter(\"message_log\"),\n",
    "            context=RunnablePassthrough()  # Pass the RAG context\n",
    "        )\n",
    "        | prompt_template\n",
    "        | chat\n",
    "        | StrOutputParser()\n",
    "    )\n",
    "\n",
    "    # Invoking the chain\n",
    "    response = chain.invoke({'question': question, 'context': context})\n",
    "\n",
    "    # Saving the interaction in memory\n",
    "    chat_memory.save_context(inputs={'input': question}, outputs={'output': response})\n",
    "\n",
    "    return response\n",
    "\n",
    "# Usage\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"Chatbot is ready! Type 'exit' to end the conversation.\")\n",
    "    while True:\n",
    "        user_input = input(\"You: \")\n",
    "        if user_input.lower() in ['exit', 'quit']:\n",
    "            break\n",
    "        response = memory_rag_chain.invoke(user_input)\n",
    "        print(f\"Taha's Bot: {response}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23eb94db-1a8c-4933-a26b-7201e1a60488",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "langchain__env",
   "language": "python",
   "name": "langchain__env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
