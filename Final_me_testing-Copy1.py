{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4a288f90-e5e0-4739-8d29-532586e85490",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "beb2202e-cc4f-4f8a-8b77-262d40536a5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "%dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": None,
   "id": "b59f2414-276c-4d67-b59d-7fa005830d25",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": None,
   "id": "5ae0a38a-9922-4882-91c1-e8f01a9025ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# necessary libraries\n",
    "from langchain.memory import ConversationSummaryMemory\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough, RunnableLambda\n",
    "from operator import itemgetter\n",
    "from langchain_core.runnables import chain\n",
    "\n",
    "# RAG setup libraries\n",
    "from langchain_openai.embeddings import OpenAIEmbeddings\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_core.documents import Document\n",
    "from langchain_text_splitters.markdown import MarkdownHeaderTextSplitter\n",
    "from langchain_text_splitters.character import CharacterTextSplitter\n",
    "from langchain_community.document_loaders import Docx2txtLoader, PyPDFLoader\n",
    "\n",
    "\n",
    "page = PyPDFLoader('User Query.pdf')\n",
    "my_document = page.load()\n",
    "\n",
    "#header_splitter = MarkdownHeaderTextSplitter(\n",
    "   # headers_to_split_on=[(\"#\", \"Course Title\"), (\"##\", \"Lecture Title\")]\n",
    "#)\n",
    "#header_splitted_document = header_splitter.split_text(my_document[0].page_content)\n",
    "#for i in range(len(header_splitted_document)):\n",
    "    #header_splitted_document[i].page_content = ' '.join(header_splitted_document[i].page_content.split())\n",
    "\n",
    "character_splitter = CharacterTextSplitter(separator=\".\", chunk_size=500, chunk_overlap=50)\n",
    "character_splitted_documents = character_splitter.split_documents(my_document)\n",
    "for i in range(len(character_splitted_documents)):\n",
    "    character_splitted_documents[i].page_content = ' '.join(character_splitted_documents[i].page_content.split())\n",
    "\n",
    "embedding = OpenAIEmbeddings(model=\"text-embedding-ada-002\")\n",
    "vector_store = Chroma.from_documents(\n",
    "    embedding=embedding,\n",
    "    documents=character_splitted_documents,\n",
    "    persist_directory=\"./TCP_directory_1\"\n",
    ")\n",
    "\n",
    "retriever = vector_store.as_retriever(search_type='mmr', search_kwargs={'k': 3, 'lambda_multi': 0.5})\n",
    "\n",
    "#chatbot memory\n",
    "chat_memory = ConversationSummaryMemory(llm=ChatOpenAI(), memory_key='message_log')\n",
    "\n",
    "#prompt template for the chatbot\n",
    "TEMPLATE = \"\"\"\n",
    "The AI should completely understand the question and only answer strictly based on the provided document context\n",
    "The AI should not talk about document in the answer.  \n",
    "The AI should not hallucinate.\n",
    "if the question does not have enough information, the AI should ask for more details.\n",
    "The AI should Deliver well-structured, polished responses that enhance the overall user experience.\n",
    "If the answer is not available in the context, the AI truthfully responds: \"Sorry, I don't know the answer.\"\n",
    "\n",
    "Current Conversation:\n",
    "{message_log}\n",
    "\n",
    "Question Context:\n",
    "{context}\n",
    "\n",
    "Human:\n",
    "{question}\n",
    "\n",
    "AI:\n",
    "\"\"\"\n",
    "prompt_template = PromptTemplate.from_template(template=TEMPLATE)\n",
    "\n",
    "# ChatOpenAI instance\n",
    "chat = ChatOpenAI(\n",
    "    model=\"gpt-4\",\n",
    "    temperature=0,\n",
    "    max_tokens=250\n",
    ")\n",
    "\n",
    "#chain combining memory and RAG\n",
    "@chain\n",
    "def memory_rag_chain(question):\n",
    "    # Retrieve relevant documents from the vector store\n",
    "    retrieved_docs = retriever.invoke(question)\n",
    "    context = \"\\n\".join([doc.page_content for doc in retrieved_docs])\n",
    "\n",
    "    # If no relevant context is found, a \"Sorry\" response is returned\n",
    "    if not context.strip():\n",
    "        response = \"Sorry, I don't know the answer.\"\n",
    "        chat_memory.save_context(inputs={'input': question}, outputs={'output': response})\n",
    "        return response\n",
    "\n",
    "    # Combining memory and RAG for the prompt\n",
    "    chain = (\n",
    "        RunnablePassthrough.assign(\n",
    "            message_log=RunnableLambda(chat_memory.load_memory_variables) | itemgetter(\"message_log\"),\n",
    "            context=RunnablePassthrough()  # Pass the RAG context\n",
    "        )\n",
    "        | prompt_template\n",
    "        | chat\n",
    "        | StrOutputParser()\n",
    "    )\n",
    "\n",
    "    # Invoking the chain\n",
    "    response = chain.invoke({'question': question, 'context': context})\n",
    "\n",
    "    # Saving the interaction in memory\n",
    "    chat_memory.save_context(inputs={'input': question}, outputs={'output': response})\n",
    "\n",
    "    return response\n",
    "\n",
    "# Usage\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"Chatbot is ready! Type 'exit' to end the conversation.\")\n",
    "    while True:\n",
    "        user_input = input(\"You: \")\n",
    "        if user_input.lower() in ['exit', 'quit']:\n",
    "            break\n",
    "        response = memory_rag_chain.invoke(user_input)\n",
    "        print(f\"Taha's Bot: {response}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": none,
   "id": "23eb94db-1a8c-4933-a26b-7201e1a60488",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "langchain__env",
   "language": "python",
   "name": "langchain__env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
